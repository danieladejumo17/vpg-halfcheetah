{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7427d35-912f-4eb2-a84e-a199c8a05bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train_vpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc171857-2b7b-4afc-bc84-4d572076f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import vpg.vpg_core as core\n",
    "from vpg.vpg_buffer import VPGBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdaa00f4-bb21-4b3c-af50-e1ff0e012a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "ENV_ID = \"HalfCheetah-v2\"\n",
    "HIDDEN_SIZES = [64, 64]  # for both pi and v\n",
    "PI_LR = 1e-3\n",
    "V_LR = 1e-3\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.98\n",
    "ACTIVATION_FN = nn.Tanh\n",
    "TRAIN_V_ITERS = 20\n",
    "MAX_EPISODE_LENGTH = 1000\n",
    "STEPS_PER_EPOCH = 4000\n",
    "EPOCHS = 1 # 500\n",
    "\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56e7433-280d-480e-bcc5-fe1a524b1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SEEDS\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8036cb01-0926-4634-88ce-e464b05602fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/miniconda3/envs/spinningup/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters. PI: 5708 V: 5377\n"
     ]
    }
   ],
   "source": [
    "# Create the env\n",
    "env = gym.make(ENV_ID)\n",
    "\n",
    "# Create the Actor-Critic\n",
    "ac = core.MLPActorCritic(env.observation_space, env.action_space, HIDDEN_SIZES, activation=nn.Tanh)\n",
    "\n",
    "# Create optimizers for the actor and critic models\n",
    "pi_optimizer = torch.optim.Adam(ac.pi.parameters(), lr=PI_LR)\n",
    "vf_optimizer = torch.optim.Adam(ac.v.parameters(), lr=V_LR)\n",
    "\n",
    "# Create the VPGBuffer Object\n",
    "buf = VPGBuffer(env.observation_space.shape, env.action_space.shape, STEPS_PER_EPOCH, GAMMA, GAE_LAMBDA)\n",
    "\n",
    "# Count Paramaters\n",
    "var_counts = tuple(core.count_params(module) for module in [ac.pi, ac.v])\n",
    "print(\"Number of Parameters. PI: {} V: {}\".format(*var_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4026e-c5a8-47e1-bf36-28a6e350bba9",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555cb3ee-e89d-4c4f-bec2-a82121d92d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect experience by running the agent in the environment for a number of steps.\n",
    "# This is called an epoch. An epoch will to transverse N trajectories of agent-environment interactions.\n",
    "# Per Epoch we'll run the agent for `STEPS_PER_EPOCH` number of steps\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "ep_rets, ep_lens = [], []\n",
    "ep_ret, ep_len = 0, 0\n",
    "\n",
    "for step in range(STEPS_PER_EPOCH):\n",
    "    # get action from the policy\n",
    "    act, val, logp = ac.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "    \n",
    "    # step through the env with the action from the policy\n",
    "    next_obs, rew, done, _ = env.step(act)\n",
    "    ep_ret += rew\n",
    "    ep_len += 1\n",
    "    \n",
    "    # store step in the VPG buffer\n",
    "    buf.store(obs, act, rew, val, logp)\n",
    "    \n",
    "    # update the obs\n",
    "    obs = next_obs\n",
    "    \n",
    "    # check for terminal or epoch end\n",
    "    timeout = (ep_len == MAX_EPISODE_LENGTH)\n",
    "    terminal = (done or timeout)\n",
    "    epoch_end = (step == (STEPS_PER_EPOCH - 1))\n",
    "    \n",
    "    # if trajectory ends or epoch ends\n",
    "    if terminal or epoch_end:\n",
    "        # Log trajectory cut-off byb epoch end\n",
    "        if epoch_end and not terminal:\n",
    "            print(f\"WARNING: Trajectory cut off by epoch end at step {ep_len}\")\n",
    "        \n",
    "        # bootstrap value target if trajectory didn't reach terminal state\n",
    "        if timeout or (epoch_end and not done):\n",
    "            _, v, _ = ac.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        else:\n",
    "            v = 0\n",
    "        \n",
    "        # Finish a trajectory\n",
    "        buf.finish_path(v)\n",
    "        \n",
    "        # only save ep_rew and ep_len if trajectory finished\n",
    "        if terminal:\n",
    "            ep_lens.append(ep_len)\n",
    "            ep_rets.append(ep_ret)\n",
    "        ep_len, ep_ret = 0, 0\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a535e4f-c165-4f08-a1ce-5fabd6dd8dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-4.3119, -4.7736, -5.2322,  ..., -4.6005, -5.5176, -6.3941]),\n",
       " tensor([-4.3119, -4.7736, -5.2322,  ..., -4.6005, -5.5176, -6.3941],\n",
       "        grad_fn=<SumBackward1>),\n",
       " tensor(0.0192, grad_fn=<NegBackward>),\n",
       " tensor(0.0192),\n",
       " -7.152557435219364e-10,\n",
       " 0.9189439415931702)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the end of an epoch perform one update step\n",
    "# An update step will perform a step of gradient ascent on the policy performance of the PI (actor) network\n",
    "# This is also equivalent to one step of gradient descent of the loss (-ve policy performance)\n",
    "# An update step will also perform N gradient descent steps to fit the value network (V) on its MSE loss\n",
    "\n",
    "# ------------------- Compute loss of PI network\n",
    "# ------------------- loss = -Mean(logp*adv)\n",
    "\n",
    "# get the observations, action taken, returns, advantages, ... for an epoch \n",
    "data = buf.get()\n",
    "\n",
    "# obs, act, ret, adv, logp = data\n",
    "obs, act, adv, logp_old = data[\"obs\"], data[\"act\"], data[\"adv\"], data[\"logp\"]\n",
    "\n",
    "pi, logp = ac.pi(obs, act)\n",
    "loss_pi = -(logp*adv).mean()\n",
    "\n",
    "# Compute KL divergence and entropy\n",
    "approx_kl = (logp_old - logp).mean().item()\n",
    "ent = pi.entropy().mean().item()\n",
    "pi_info = dict(kl=approx_kl, ent= ent)\n",
    "\n",
    "logp_old, logp, loss_pi, -(logp_old*adv).mean(), approx_kl, ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f51c9247-082a-4e97-9dd7-16a48e80485b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.8610e-06, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logp_old - logp).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1fc897b-9fac-435b-903e-812813eea9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(993.5066, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------- Compute the loss of the value function \n",
    "obs, ret = data[\"obs\"], data[\"ret\"]\n",
    "loss_v = ((ac.v(obs) - ret)**2).mean() # the MSE of the V prediction and the actual reward-to-go\n",
    "loss_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d764b1-2705-4178-a573-bd45c1a0c0ac",
   "metadata": {},
   "source": [
    "Define a function to compute loss of the pi network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcc282b-5ac4-48ed-8bd6-15920e79aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_loss_pi(data, ac):\n",
    "    # obs, act, ret, adv, logp = data\n",
    "    obs, act, adv, logp_old = data[\"obs\"], data[\"act\"], data[\"adv\"], data[\"logp\"]\n",
    "\n",
    "    pi, logp = ac.pi(obs, act)\n",
    "    loss_pi = -(logp*adv).mean()\n",
    "\n",
    "    # Compute KL divergence and entropy\n",
    "    approx_kl = (logp_old - logp).mean().item()\n",
    "    ent = pi.entropy().mean().item()\n",
    "    pi_info = dict(kl=approx_kl, ent=ent)\n",
    "    \n",
    "    return loss_pi, pi_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb6434-92ca-4734-aa40-dec3403d689c",
   "metadata": {},
   "source": [
    "Define a function to compute the loss of the value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6514ec87-b465-49e0-9a91-6db190e9a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_loss_v(data, ac):\n",
    "    # obs, act, ret, adv, logp = data\n",
    "    obs, ret = data[\"obs\"], data[\"ret\"]\n",
    "    return ((ac.v(obs) - ret)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260837f7-419f-41ad-9cfb-9d347d991196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Value function loss: 993.506591796875\n",
      "Final Value function loss: 963.981689453125\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient descent on the loss of the PI and Value networks\n",
    "\n",
    "pi_optimizer.zero_grad()\n",
    "loss_p, pi_info = compute_loss_pi(data, ac)\n",
    "loss_p.backward()\n",
    "pi_optimizer.step()\n",
    "# the loss of the PI network is only valid for one gradient descent step, on the data generated by that \n",
    "# policy (the previous parameters). The value of the loss cannot be use to track the performance of the\n",
    "# policy\n",
    "\n",
    "print(f\"Old Value function loss: {compute_loss_v(data, ac)}\")\n",
    "for _  in range(TRAIN_V_ITERS):\n",
    "    vf_optimizer.zero_grad()\n",
    "    loss_v = compute_loss_v(data, ac)\n",
    "    loss_v.backward()\n",
    "    vf_optimizer.step()\n",
    "print(f\"Final Value function loss: {compute_loss_v(data, ac)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8b48d-c098-44f0-8518-60bb6af0e9e3",
   "metadata": {},
   "source": [
    "Define a function to update the policy and the value function at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b154c4-837e-466d-8320-29bf04e5f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def update(data, ac, pi_optimizer, vf_optimizer, train_v_iters):\n",
    "    # Get loss and info values before update\n",
    "    loss_pi_old, pi_info_old = compute_loss_pi(data, ac)\n",
    "    loss_pi_old = loss_pi_old.item()\n",
    "    loss_v_old = compute_loss_v(data, ac).item()\n",
    "    \n",
    "    # Train the policy with a single step of gradient descent\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi, pi_info = compute_loss_pi(data, ac)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Fit value function\n",
    "    for _  in range(train_v_iters):\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_v = compute_loss_v(data, ac)\n",
    "        loss_v.backward()\n",
    "        vf_optimizer.step()\n",
    "        \n",
    "    # Log changes from update\n",
    "    kl, ent = pi_info[\"kl\"], pi_info_old[\"ent\"]\n",
    "    return dict(LossPi=loss_pi_old, LossV=loss_v_old, KL=kl, Entropy=ent, \n",
    "                DeltaLossPi=loss_pi.item() - loss_pi_old, DeltaLossV=loss_v.item() - loss_v_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55d35bb8-0457-4f00-a090-cae731aaa7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LossPi': 0.003244533436372876,\n",
       " 'LossV': 963.981689453125,\n",
       " 'KL': 0.010360285639762878,\n",
       " 'Entropy': 0.9192747473716736,\n",
       " 'DeltaLossPi': 0.0,\n",
       " 'DeltaLossV': -57.24359130859375}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update(data, ac, pi_optimizer, vf_optimizer, TRAIN_V_ITERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b15ae-c7af-4150-b5f5-3db58b51f065",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced0405-8b6b-4c38-820c-f25593a93a17",
   "metadata": {},
   "source": [
    "Define the train_vpg function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "120c30ee-ea78-4636-b1f0-b3b7e04fb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_vpg(env_fn, actor_critic, ac_kwargs, pi_lr, vf_lr,\n",
    "              epochs, steps_per_epoch, gamma, gae_lambda, train_v_iters,\n",
    "              max_ep_len, log_freq=10, seed=0, exp_name=\"vpg\"):\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create the training environment\n",
    "    env = env_fn()\n",
    "    \n",
    "    # Create the actor-critic\n",
    "    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "    param_counts = tuple(core.count_params(module) for module in [ac.pi, ac.v])\n",
    "    \n",
    "    \n",
    "    # Create optimizers for the policy and value function\n",
    "    pi_optimizer = torch.optim.Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = torch.optim.Adam(ac.v.parameters(), lr=vf_lr)\n",
    "    \n",
    "    # Create the VPG Buffer\n",
    "    buffer = VPGBuffer(env.observation_space.shape, env.action_space.shape, \n",
    "                       steps_per_epoch, gamma, gae_lambda)\n",
    "    \n",
    "    # Tensorboard Writer\n",
    "    writer = SummaryWriter(f\"./logs/tensorboard/{exp_name}\")\n",
    "    \n",
    "    # Run `epochs` number of epochs\n",
    "    obs, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_rets, epoch_lens = [], []\n",
    "        for step in range(steps_per_epoch):\n",
    "            # get action from the policy\n",
    "            act, val, logp = ac.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "\n",
    "            # step through the env with the action from the policy\n",
    "            next_obs, rew, done, _ = env.step(act)\n",
    "            ep_ret += rew\n",
    "            ep_len += 1\n",
    "\n",
    "            # store step in the VPG buffer\n",
    "            buffer.store(obs, act, rew, val, logp)\n",
    "\n",
    "            # update the obs\n",
    "            obs = next_obs\n",
    "\n",
    "            # check for terminal or epoch end\n",
    "            timeout = (ep_len == max_ep_len)\n",
    "            terminal = (done or timeout)\n",
    "            epoch_end = (step == (steps_per_epoch - 1))\n",
    "\n",
    "            # if trajectory ends or epoch ends\n",
    "            if terminal or epoch_end:\n",
    "                # Log trajectory cut-off byb epoch end\n",
    "                if epoch_end and not terminal:\n",
    "                    pass\n",
    "                    # print(f\"WARNING: Trajectory cut off by epoch end at step {ep_len}\")\n",
    "\n",
    "                # bootstrap value target if trajectory didn't reach terminal state\n",
    "                if timeout or epoch_end: # change to if not done\n",
    "                    _, v, _ = ac.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "\n",
    "                # Finish a trajectory\n",
    "                buffer.finish_path(v)\n",
    "\n",
    "                # only save ep_rew and ep_len if trajectory finished\n",
    "                if terminal:\n",
    "                    epoch_lens.append(ep_len)\n",
    "                    epoch_rets.append(ep_ret)\n",
    "                obs, ep_len, ep_ret = env.reset(), 0, 0\n",
    "            \n",
    "        # Perform VPG update\n",
    "        data = buffer.get()\n",
    "        res = update(data, ac, pi_optimizer, vf_optimizer, train_v_iters)\n",
    "        \n",
    "        # Log Result\n",
    "        if (epoch % log_freq == 0) or (epoch == epochs - 1):\n",
    "            print(f\"Epoch: {epoch} Mean Reward: {np.mean(epoch_rets):.2f}, Mean Length: {np.mean(epoch_lens):.1f} LossV: {res['LossV']:.3f}\")\n",
    "            \n",
    "        writer.add_scalar(\"Mean Return\", np.mean(epoch_rets), global_step=epoch)\n",
    "        writer.add_scalar(\"Mean Length\", np.mean(epoch_lens), global_step=epoch)\n",
    "        writer.add_scalar(\"Value Loss\", res['LossV'], global_step=epoch)\n",
    "    \n",
    "    return ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9860b5f4-2993-4498-848e-13f589f5ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "train_kwargs = {\"env_fn\": lambda: gym.make(ENV_ID), \n",
    "                \"actor_critic\": core.MLPActorCritic,\n",
    "                \"ac_kwargs\": {\"hidden_sizes\": HIDDEN_SIZES, \"activation\": ACTIVATION_FN},\n",
    "                \"pi_lr\": PI_LR,\n",
    "                \"vf_lr\": V_LR,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"steps_per_epoch\": STEPS_PER_EPOCH,\n",
    "                \"gamma\": GAMMA,\n",
    "                \"gae_lambda\": GAE_LAMBDA,\n",
    "                \"train_v_iters\": TRAIN_V_ITERS,\n",
    "                \"max_ep_len\": MAX_EPISODE_LENGTH, \n",
    "                \"log_freq\": 10, \n",
    "                \"seed\": SEED, \n",
    "                \"exp_name\": f\"vpg_{ENV_ID}_{time.time()}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "848b8667-3593-4358-910d-48d4d3a6756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Mean Reward: -293.87, Mean Length: 1000.0 LossV: 939.325\n"
     ]
    }
   ],
   "source": [
    "model = train_vpg(**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bbf06bb-7053-4b1f-b580-2912a4a661b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE MODEL\n",
    "torch.save(model.state_dict(), f\"./logs/checkpoints/{ENV_ID}_{time.time()}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c0bedbf-d9bf-461a-9368-237a0d2773a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD A SAVED\n",
    "ac_kwargs = {\"hidden_sizes\": HIDDEN_SIZES, \"activation\": ACTIVATION_FN}\n",
    "model = core.MLPActorCritic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"./logs/checkpoints/{ENV_ID}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b2f580f-3c75-4157-9f73-92fdf4f8e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RECORD AGENT IN ENVIRONMENT\n",
    "# from colabgymrender.recorder import Recorder\n",
    "\n",
    "# eval_env = gym.make(ENV_ID)\n",
    "# env = Recorder(env, \"./logs/videos\", fps=30)\n",
    "\n",
    "# NUM_EPISODES = 10\n",
    "# done = False\n",
    "# obs = env.reset()\n",
    "\n",
    "# for _ in range(NUM_EPISODES):\n",
    "#     while not done:\n",
    "#         act = model.act(torch.as_tensor(obs, dtype=torch.float32))\n",
    "#         obs, rew, done, _ = env.step(act)\n",
    "#     done = False\n",
    "#     obs = env.reset()\n",
    "# # env.close()\n",
    "# env.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deba3231-dbf1-4cff-b620-f7b2b42426d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT VIDEO\n",
    "# video_path attempt to get the last recorded video.\n",
    "# If video_path is not correct, provide the correct path\n",
    "# video_path = sorted(os.listdir(\"./logs/videos\"), reverse=True)[0]\n",
    "\n",
    "# !ffmpeg -i {video_path} -vcodec h264 replay.mp4\n",
    "# !rm {video_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dbaac00-9895-4e40-9663-495b337e5375",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev\n",
    "\n",
    "# nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5902764af3eca108f12d4a47809553b6d971d00f185bccc8721f2411d4980bf3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
